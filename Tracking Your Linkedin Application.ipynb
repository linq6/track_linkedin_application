{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking your Linkedin Applications\n",
    "The following code firstly create a new csv file to record the job title, company name, salary, location, and application dates.\n",
    "NOTE: You need to input your username and password for your linkedin account. YOU DO NOT NEED TO run the following two blocks after you have already created a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "from scrapping_functions import login, scrape_page, scrape_all_pages\n",
    "\n",
    "USERNAME = 'INPUT_YOUR_USERNAME'\n",
    "PASSWORD = 'INPUT_YOUR_PASSWORD'\n",
    "\n",
    "# LinkedIn URLs\n",
    "LOGIN_URL = 'https://www.linkedin.com/login'\n",
    "JOB_URL = 'https://www.linkedin.com/my-items/saved-jobs/?cardType=APPLIED'\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "login(driver, USERNAME, PASSWORD)\n",
    "time.sleep(2)\n",
    "driver.get(JOB_URL)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save the data we just scrapped from LinkedIn. Here I assume that when you first time run this, you will obtain a lot of previous job applications, so I set the column of `Application Date` to a string ''Before''. Insert your own directory to store the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = scrape_all_pages(driver)\n",
    "all_data['Application Date'] = 'Before'\n",
    "print(all_data)\n",
    "YOUR_DIRECTORY = #e.g. Desktop/job_applications.csv\n",
    "all_data.to_csv(YOUR_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will update the form at a daily base. You need to input your Linkedin login info and csv directory as before. I set application date to the date you run the code. So to make the application date accurate, remember to run this code at the end of every day when you made application!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import pandas as pd\n"
    "from scrapping_functions import login, scrape_page\n",
    "from selenium import webdriver\n"
    "\n",
    "# Load existing data\n",
    "all_data = pd.read_csv(YOUR_DIRECTORY)\n",
    "last_job = all_data.iloc[0,:]\n",
    "\n",
    "USERNAME = 'INPUT_YOUR_USERNAME'\n",
    "PASSWORD = 'INPUT_YOUR_PASSWORD'\n",
    "# Initialize cut_off variable\n",
    "cut_off = None\n",
    "LOGIN_URL = 'https://www.linkedin.com/login'\n",
    "JOB_URL = 'https://www.linkedin.com/my-items/saved-jobs/?cardType=APPLIED'\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "login(driver, USERNAME, PASSWORD)\n",
    "time.sleep(2)\n",
    "driver.get(JOB_URL)\n",
    "time.sleep(2)\n",
    "\n",
    "\n",
    "while cut_off is None:\n",
    "    new_data = scrape_page(driver)\n",
    "    \n",
    "    for i in range(new_data.shape[0]):\n",
    "        if new_data.iloc[i]['Job Title'] == last_job['Job Title'] and new_data.iloc[i]['Company Name'] == last_job['Company Name']:\n",
    "            cut_off = i\n",
    "            print(f'cut_off is {i}')\n",
    "            break\n",
    "\n",
    "    if cut_off is not None:\n",
    "        new_data = new_data.iloc[:cut_off, :]\n",
    "        new_data['Application Date'] = date.today()\n",
    "        print(new_data)\n",
    "        all_data = pd.concat([new_data, all_data], ignore_index=True)\n",
    "\n",
    "    else:\n",
    "        new_data['Application Date'] = date.today()\n",
    "        all_data = pd.concat([new_data, all_data], ignore_index=True)\n",
    "\n",
    "        try:\n",
    "            next_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//button[contains(@aria-label, \"Next\")]'))\n",
    "            )\n",
    "            next_button.click()\n",
    "            time.sleep(3)  # Wait for the next page to load\n",
    "        except Exception as e:\n",
    "            print(f\"Could not navigate to the next page: {e}\")\n",
    "            break\n",
    "\n",
    "# Print the final concatenated DataFrame\n",
    "print(all_data)\n",
    "\n",
    "# Optionally save the updated data to a CSV file\n",
    "all_data.to_csv(YOUR_DIRECTORY, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
